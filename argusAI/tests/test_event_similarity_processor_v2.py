"""
Tests for Event Similarity Processor V2

Test coverage:
1. Initialization and configuration
2. Event processing workflow
3. Batch processing
4. Error handling
5. Shutdown procedures
"""

import logging
import time
from datetime import datetime
from threading import Event
from unittest.mock import Mock, MagicMock, patch
from uuid import uuid4

import pytest

from argus.backend.models.argus_ai import SCTErrorEventEmbedding, SCTCriticalEventEmbedding
from argus.backend.plugins.sct.testrun import SCTUnprocessedEvent
from argusAI.event_similarity_processor_v2 import EventSimilarityProcessorV2, BgeSmallEnEmbeddingModel, SimilarEvent


LOGGER = logging.getLogger(__name__)


class TestBgeSmallEnEmbeddingModel:
    """Tests for BGE-Small-EN embedding model configuration."""

    def test_model_should_have_correct_configuration(self):
        """Model configuration should match BGE-Small-EN specifications."""
        assert BgeSmallEnEmbeddingModel.MODEL_NAME == "bge-small-en-v1.5"
        assert "bge-small-en-v1.5" in str(BgeSmallEnEmbeddingModel.DOWNLOAD_PATH)
        assert BgeSmallEnEmbeddingModel.EXTRACTED_FOLDER_NAME == "onnx"
        assert BgeSmallEnEmbeddingModel.ARCHIVE_FILENAME == "onnx.tar.gz"
        assert "s3.us-east-1.amazonaws.com" in BgeSmallEnEmbeddingModel.MODEL_DOWNLOAD_URL
        assert BgeSmallEnEmbeddingModel._MODEL_SHA256 is not None


class TestEventSimilarityProcessorV2Initialization:
    """Tests for processor initialization."""

    @patch("argusAI.event_similarity_processor_v2.ScyllaConnection")
    @patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel")
    @patch("argusAI.event_similarity_processor_v2.MessageSanitizer")
    def test_initialization_should_create_processor_with_dependencies(
        self, mock_sanitizer, mock_embedding_model, mock_scylla_connection
    ):
        """Processor should initialize with embedding model, sanitizer, and database connection."""
        processor = EventSimilarityProcessorV2()

        assert processor.embedding_model is not None
        assert processor.sanitizer is not None
        assert processor.db is not None
        assert processor.processed_count == 0
        assert processor.error_count == 0
        assert processor.stop_event is not None
        assert not processor.stop_event.is_set()

    @patch("argusAI.event_similarity_processor_v2.ScyllaConnection")
    @patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel")
    @patch("argusAI.event_similarity_processor_v2.MessageSanitizer")
    def test_initialization_should_accept_custom_stop_event(
        self, mock_sanitizer, mock_embedding_model, mock_scylla_connection
    ):
        """Processor should accept a custom stop event for thread coordination."""
        custom_stop_event = Event()
        processor = EventSimilarityProcessorV2(stop_event=custom_stop_event)

        assert processor.stop_event is custom_stop_event


class TestEventProcessing:
    """Tests for event processing functionality."""

    @pytest.fixture
    def mock_db(self):
        """Mock database connection."""
        mock = MagicMock()
        return mock

    @pytest.fixture
    def mock_embedding_model(self):
        """Mock embedding model that returns fake embeddings."""
        mock = MagicMock()
        mock.return_value = [[0.1, 0.2, 0.3] * 128]  # 384-dim vector
        return mock

    @pytest.fixture
    def mock_sanitizer(self):
        """Mock message sanitizer."""
        mock = MagicMock()
        mock.sanitize.return_value = "sanitized message"
        return mock

    @pytest.fixture
    def processor(self, mock_db, mock_embedding_model, mock_sanitizer):
        """Create processor with mocked dependencies."""
        with (
            patch("argusAI.event_similarity_processor_v2.ScyllaConnection", return_value=mock_db),
            patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel", return_value=mock_embedding_model),
            patch("argusAI.event_similarity_processor_v2.MessageSanitizer", return_value=mock_sanitizer),
        ):
            processor = EventSimilarityProcessorV2()
            return processor

    def test_process_single_event_should_complete_full_workflow_for_error_event(self, processor):
        """Processing an ERROR event should sanitize, embed, store, and cleanup."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()
        original_message = "Error: Connection failed to 192.168.1.1"

        # Mock database responses
        mock_event = Mock()
        mock_event.message = original_message
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        # Execute
        processor._process_single_event(run_id, severity, ts)

        # Verify SELECT query was executed
        select_calls = [call for call in processor.db.execute.call_args_list if call[0][0].startswith("SELECT message")]
        assert len(select_calls) == 1
        assert select_calls[0][0][1] == (run_id, severity, ts)

        # Verify sanitizer was called
        processor.sanitizer.sanitize.assert_called_once_with(run_id, original_message)

        # Verify embedding was generated
        processor.embedding_model.assert_called_once_with(["sanitized message"])

        # Verify INSERT into sct_error_event_embedding
        insert_calls = [
            call
            for call in processor.db.execute.call_args_list
            if call[0][0].startswith(f"INSERT INTO {SCTErrorEventEmbedding.__table_name__}")
        ]
        assert len(insert_calls) == 1

        # Verify DELETE from unprocessed_events
        delete_calls = [
            call
            for call in processor.db.execute.call_args_list
            if call[0][0].startswith(f"DELETE FROM {SCTUnprocessedEvent.__table_name__}")
        ]
        assert len(delete_calls) == 1
        assert delete_calls[0][0][1] == (run_id, severity, ts)

    def test_process_single_event_should_complete_full_workflow_for_critical_event(self, processor):
        """Processing a CRITICAL event should store in sct_critical_event_embedding table."""
        run_id = uuid4()
        severity = "CRITICAL"
        ts = datetime.now()
        original_message = "Critical: Cluster is down"

        # Mock database responses
        mock_event = Mock()
        mock_event.message = original_message
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        # Execute
        processor._process_single_event(run_id, severity, ts)

        # Verify INSERT into sct_critical_event_embedding
        insert_calls = [
            call
            for call in processor.db.execute.call_args_list
            if call[0][0].startswith(f"INSERT INTO {SCTCriticalEventEmbedding.__table_name__}")
        ]
        assert len(insert_calls) == 1

    def test_process_single_event_should_raise_error_when_event_not_found(self, processor):
        """Processing should raise ValueError when event is not found in database."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        # Mock event not found
        mock_result = Mock()
        mock_result.one.return_value = None
        processor.db.execute.return_value = mock_result

        # Execute and verify
        with pytest.raises(ValueError, match="Event not found"):
            processor._process_single_event(run_id, severity, ts)

    def test_process_single_event_should_raise_error_when_message_is_empty(self, processor):
        """Processing should raise ValueError when event message is empty."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        # Mock event with empty message
        mock_event = Mock()
        mock_event.message = ""
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        # Execute and verify
        with pytest.raises(ValueError, match="Event message is empty"):
            processor._process_single_event(run_id, severity, ts)

    def test_process_single_event_should_raise_error_when_sanitized_message_is_empty(self, processor):
        """Processing should raise ValueError when sanitized message is empty."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        # Mock event with message
        mock_event = Mock()
        mock_event.message = "Some message"
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        # Mock sanitizer returning empty string
        processor.sanitizer.sanitize.return_value = "   "

        # Execute and verify
        with pytest.raises(ValueError, match="Sanitized message is empty"):
            processor._process_single_event(run_id, severity, ts)

    def test_process_single_event_should_raise_error_for_unsupported_severity(self, processor):
        """Processing should raise ValueError for unsupported severity levels."""
        run_id = uuid4()
        severity = "WARNING"  # Unsupported severity
        ts = datetime.now()

        # Mock event
        mock_event = Mock()
        mock_event.message = "Warning message"
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        # Execute and verify
        with pytest.raises(ValueError, match="Unsupported severity"):
            processor._process_single_event(run_id, severity, ts)


class TestBatchProcessing:
    """Tests for batch processing functionality."""

    @pytest.fixture
    def processor_with_mocks(self):
        """Create processor with all dependencies mocked."""
        with (
            patch("argusAI.event_similarity_processor_v2.ScyllaConnection") as mock_scylla,
            patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel") as mock_embedding,
            patch("argusAI.event_similarity_processor_v2.MessageSanitizer") as mock_sanitizer,
        ):
            # Setup mocks
            mock_db = MagicMock()
            mock_scylla.return_value = mock_db
            mock_embedding_instance = MagicMock()
            mock_embedding_instance.return_value = [[0.1] * 384]
            mock_embedding.return_value = mock_embedding_instance
            mock_sanitizer_instance = MagicMock()
            mock_sanitizer_instance.sanitize.return_value = "sanitized"
            mock_sanitizer.return_value = mock_sanitizer_instance

            processor = EventSimilarityProcessorV2()
            return processor

    def test_process_batch_should_return_zero_when_no_events(self, processor_with_mocks):
        """Batch processing should return 0 when no unprocessed events exist."""
        # Mock empty result
        processor_with_mocks.db.execute.return_value = []

        result = processor_with_mocks._process_batch()

        assert result == 0

    def test_process_batch_should_process_multiple_events(self, processor_with_mocks):
        """Batch processing should process multiple events and update counters."""
        # Create mock events
        mock_event1 = Mock(run_id=uuid4(), severity="ERROR", ts=datetime.now())
        mock_event2 = Mock(run_id=uuid4(), severity="CRITICAL", ts=datetime.now())
        mock_event3 = Mock(run_id=uuid4(), severity="ERROR", ts=datetime.now())

        # Setup mock responses for unprocessed events query
        processor_with_mocks.db.execute.return_value = [mock_event1, mock_event2, mock_event3]

        # Mock _process_single_event to avoid actual processing
        with patch.object(processor_with_mocks, "_process_single_event") as mock_process:
            result = processor_with_mocks._process_batch()

        assert result == 3
        assert processor_with_mocks.processed_count == 3
        assert mock_process.call_count == 3

    def test_process_batch_should_handle_individual_event_failures(self, processor_with_mocks):
        """Batch processing should continue despite individual event failures and clean up failed events."""
        # Create mock events
        mock_event1 = Mock(run_id=uuid4(), severity="ERROR", ts=datetime.now())
        mock_event2 = Mock(run_id=uuid4(), severity="CRITICAL", ts=datetime.now())

        # Setup mock responses
        processor_with_mocks.db.execute.return_value = [mock_event1, mock_event2]

        # Mock _process_single_event to fail on first event
        with patch.object(processor_with_mocks, "_process_single_event") as mock_process:
            mock_process.side_effect = [Exception("Processing failed"), None]
            result = processor_with_mocks._process_batch()

        assert result == 1  # Only one successful
        assert processor_with_mocks.processed_count == 1
        assert processor_with_mocks.error_count == 1

    def test_process_batch_should_respect_batch_size(self, processor_with_mocks):
        """Batch processing should use the specified batch size."""
        processor_with_mocks.db.execute.return_value = []

        processor_with_mocks._process_batch(batch_size=50)

        # Verify LIMIT clause in query
        call_args = processor_with_mocks.db.execute.call_args[0][0]
        assert "LIMIT 50" in call_args


class TestProcessingLoop:
    """Tests for main processing loop."""

    @pytest.fixture
    def processor_with_stop_event(self):
        """Create processor with stop event for testing."""
        with (
            patch("argusAI.event_similarity_processor_v2.ScyllaConnection"),
            patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel"),
            patch("argusAI.event_similarity_processor_v2.MessageSanitizer"),
        ):
            stop_event = Event()
            processor = EventSimilarityProcessorV2(stop_event=stop_event)
            return processor, stop_event

    @patch("argusAI.event_similarity_processor_v2.time.sleep")
    def test_processing_loop_should_stop_when_stop_event_is_set(self, mock_sleep, processor_with_stop_event):
        """Processing loop should exit when stop_event is set."""
        processor, stop_event = processor_with_stop_event

        # Mock _process_batch to return 0 (no events)
        with patch.object(processor, "_process_batch", return_value=0):
            # Set stop event after one iteration
            def set_stop_after_sleep(*args):
                stop_event.set()

            mock_sleep.side_effect = set_stop_after_sleep

            processor.process_unprocessed_events()

            # Verify sleep was called (indicating loop ran)
            assert mock_sleep.called

    @patch("argusAI.event_similarity_processor_v2.time.sleep")
    def test_processing_loop_should_sleep_when_no_events(self, mock_sleep, processor_with_stop_event):
        """Processing loop should sleep when no events are available."""
        processor, stop_event = processor_with_stop_event

        iterations = 0

        def count_iterations(*args):
            nonlocal iterations
            iterations += 1
            if iterations >= 2:
                stop_event.set()

        mock_sleep.side_effect = count_iterations

        with patch.object(processor, "_process_batch", return_value=0):
            processor.process_unprocessed_events()

        # Verify sleep was called multiple times
        assert mock_sleep.call_count >= 2

    @patch("argusAI.event_similarity_processor_v2.time.sleep")
    def test_processing_loop_should_not_sleep_when_events_processed(self, mock_sleep, processor_with_stop_event):
        """Processing loop should continue immediately when events are processed."""
        processor, stop_event = processor_with_stop_event

        iterations = 0

        def stop_after_iterations(*args):
            nonlocal iterations
            iterations += 1
            if iterations >= 2:
                stop_event.set()
            return 5  # Simulate processing 5 events

        with patch.object(processor, "_process_batch", side_effect=stop_after_iterations):
            processor.process_unprocessed_events()

        # Sleep should not be called when events are being processed
        assert mock_sleep.call_count == 0

    @patch("argusAI.event_similarity_processor_v2.time.sleep")
    def test_processing_loop_should_handle_exceptions_and_continue(self, mock_sleep, processor_with_stop_event):
        """Processing loop should handle exceptions, increment error counter, and continue."""
        processor, stop_event = processor_with_stop_event

        iterations = 0

        def fail_then_succeed(*args):
            nonlocal iterations
            iterations += 1
            if iterations == 1:
                raise Exception("Database connection lost")
            stop_event.set()
            return 0

        with patch.object(processor, "_process_batch", side_effect=fail_then_succeed):
            processor.process_unprocessed_events()

        assert processor.error_count == 1
        assert iterations == 2  # Loop continued after error


class TestShutdown:
    """Tests for processor shutdown."""

    @patch("argusAI.event_similarity_processor_v2.ScyllaConnection")
    @patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel")
    @patch("argusAI.event_similarity_processor_v2.MessageSanitizer")
    def test_shutdown_should_set_stop_event(self, mock_sanitizer, mock_embedding, mock_scylla):
        """Shutdown should set the stop event."""
        mock_db = MagicMock()
        mock_scylla.return_value = mock_db

        processor = EventSimilarityProcessorV2()
        processor.shutdown()

        assert processor.stop_event.is_set()

    @patch("argusAI.event_similarity_processor_v2.ScyllaConnection")
    @patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel")
    @patch("argusAI.event_similarity_processor_v2.MessageSanitizer")
    def test_shutdown_should_cleanup_database_connection(self, mock_sanitizer, mock_embedding, mock_scylla):
        """Shutdown should properly close database connection."""
        mock_db = MagicMock()
        mock_scylla.return_value = mock_db

        processor = EventSimilarityProcessorV2()
        processor.shutdown()

        mock_db.shutdown.assert_called_once()


class TestIntegrationScenarios:
    """Integration-style tests for complete scenarios."""

    @pytest.fixture
    def full_processor(self):
        """Create processor with all mocks configured for full workflow."""
        with (
            patch("argusAI.event_similarity_processor_v2.ScyllaConnection") as mock_scylla,
            patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel") as mock_embedding,
            patch("argusAI.event_similarity_processor_v2.MessageSanitizer") as mock_sanitizer,
        ):
            # Setup realistic mocks
            mock_db = MagicMock()
            mock_scylla.return_value = mock_db

            mock_embedding_instance = MagicMock()
            mock_embedding_instance.return_value = [[0.1] * 384]
            mock_embedding.return_value = mock_embedding_instance

            mock_sanitizer_instance = MagicMock()
            mock_sanitizer_instance.sanitize.return_value = "sanitized event message"
            mock_sanitizer.return_value = mock_sanitizer_instance

            processor = EventSimilarityProcessorV2()
            return processor

    def test_full_workflow_should_process_error_event_successfully(self, full_processor):
        """Complete workflow should process ERROR event from start to finish."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        # Mock database responses
        mock_event = Mock(message="Connection failed to node")
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        full_processor.db.execute.return_value = mock_result

        # Execute
        full_processor._process_single_event(run_id, severity, ts)

        # Verify all steps occurred in correct order
        assert full_processor.db.execute.call_count >= 3  # SELECT, INSERT, DELETE
        assert full_processor.sanitizer.sanitize.called
        assert full_processor.embedding_model.called

    def test_batch_processing_should_handle_mixed_severities(self, full_processor):
        """Batch should successfully process mix of ERROR and CRITICAL events."""
        # Create mixed events
        events = [
            Mock(run_id=uuid4(), severity="ERROR", ts=datetime.now()),
            Mock(run_id=uuid4(), severity="CRITICAL", ts=datetime.now()),
            Mock(run_id=uuid4(), severity="ERROR", ts=datetime.now()),
        ]

        # First call returns events, subsequent calls for each event processing
        mock_event_data = Mock(message="Test message")
        mock_result = Mock()
        mock_result.one.return_value = mock_event_data

        def execute_side_effect(query, params=None):
            if "SELECT run_id" in query or "LIMIT" in query:
                return events
            return mock_result

        full_processor.db.execute.side_effect = execute_side_effect

        # Execute
        result = full_processor._process_batch()

        assert result == 3
        assert full_processor.processed_count == 3


class TestDeduplication:
    """Tests for event deduplication logic."""

    @pytest.fixture
    def processor(self):
        """Create processor with mocked dependencies."""
        with (
            patch("argusAI.event_similarity_processor_v2.ScyllaConnection") as mock_scylla,
            patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel") as mock_embedding,
            patch("argusAI.event_similarity_processor_v2.MessageSanitizer") as mock_sanitizer,
        ):
            mock_db = MagicMock()
            mock_scylla.return_value = mock_db
            mock_embedding.return_value = MagicMock(return_value=[[0.1] * 384])
            mock_sanitizer.return_value = MagicMock(sanitize=MagicMock(return_value="sanitized"))
            processor = EventSimilarityProcessorV2()
            return processor

    def test_mark_event_is_duplicate_should_return_false_when_no_rows_returned(self, processor):
        """Should return False when ANN search returns no rows (no similar events in DB or cache)."""
        run_id = uuid4()
        ts = datetime.now()
        embedding = [0.1] * 384

        # ANN query returns no results
        processor.db.session.execute.return_value = []

        result = processor._mark_event_is_duplicate(run_id, ts, "ERROR", embedding)

        assert result is False

    def test_mark_event_is_duplicate_should_return_false_when_similar_embedding_belongs_to_different_run(
        self, processor
    ):
        """Should return False when near-identical embedding exists but belongs to a different run."""
        run_id = uuid4()
        other_run_id = uuid4()
        ts = datetime.now()
        # Identical unit vectors — cosine distance would be ~0
        embedding = [1.0] + [0.0] * 383

        # DB returns a row with the same embedding but for a different run
        other_run_row = SimilarEvent(run_id=other_run_id, ts=datetime.now(), embedding=embedding, added_ts=time.time())
        processor.db.session.execute.return_value = [other_run_row]

        result = processor._mark_event_is_duplicate(run_id, ts, "ERROR", embedding)

        assert result is False

    def test_mark_event_is_duplicate_should_return_false_when_cosine_distance_too_large(self, processor):
        """Should return False when the closest embedding exceeds the similarity threshold."""
        run_id = uuid4()
        ts = datetime.now()
        # Two orthogonal unit vectors — cosine distance = 1.0, well outside (-0.1, 0.1)
        embedding = [1.0] + [0.0] * 383
        different_embedding = [0.0, 1.0] + [0.0] * 382

        existing_row = SimilarEvent(
            run_id=run_id, ts=datetime.now(), embedding=different_embedding, added_ts=time.time()
        )
        processor.db.session.execute.return_value = [existing_row]

        result = processor._mark_event_is_duplicate(run_id, ts, "ERROR", embedding)

        assert result is False

    def test_mark_event_is_duplicate_should_return_true_for_near_identical_embedding(self, processor):
        """Should return True and write duplicate_id when a near-identical embedding exists in the same run."""
        run_id = uuid4()
        ts = datetime.now()
        ts_existing = datetime.now()
        existing_event_id = uuid4()
        # Same unit vector — cosine distance = 0, clearly within (-0.1, 0.1)
        embedding = [1.0] + [0.0] * 383

        existing_row = SimilarEvent(run_id=run_id, ts=ts_existing, embedding=embedding, added_ts=time.time())

        # First session.execute call: ANN query → returns [existing_row]
        # Second session.execute call: SELECT event_id → returns a mock dupe
        mock_dupe = Mock(event_id=existing_event_id)
        ann_result_exhausted = False

        def session_execute_side_effect(*args, **kwargs):
            nonlocal ann_result_exhausted
            if not ann_result_exhausted:
                ann_result_exhausted = True
                return [existing_row]
            return Mock(one=Mock(return_value=mock_dupe))

        processor.db.session.execute.side_effect = session_execute_side_effect

        result = processor._mark_event_is_duplicate(run_id, ts, "ERROR", embedding)

        assert result is True

        # Verify unprocessed event was deleted
        delete_calls = [call for call in processor.db.execute.call_args_list if "DELETE FROM" in call[0][0]]
        assert len(delete_calls) == 1

        # Verify duplicate_id was written back to SCTEvent
        update_calls = [call for call in processor.db.execute.call_args_list if "UPDATE" in call[0][0]]
        assert len(update_calls) == 1
        assert update_calls[0][0][1][0] == existing_event_id

    def test_process_single_event_should_skip_insert_and_delete_when_duplicate_detected(self, processor):
        """_process_single_event should return early without INSERT or final DELETE when a duplicate is found."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        mock_event = Mock(message="Connection refused to 10.0.0.1")
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        with patch.object(processor, "_mark_event_is_duplicate", return_value=True):
            processor._process_single_event(run_id, severity, ts)

        insert_calls = [call for call in processor.db.execute.call_args_list if "INSERT INTO" in call[0][0]]
        assert len(insert_calls) == 0

        # The only DELETE should come from _mark_event_is_duplicate itself (mocked), not from step 5
        delete_calls = [
            call
            for call in processor.db.execute.call_args_list
            if call[0][0].startswith(f"DELETE FROM {SCTUnprocessedEvent.__table_name__}")
        ]
        assert len(delete_calls) == 0

    def test_mark_event_is_duplicate_should_propagate_exceptions(self, processor):
        """Exceptions raised during duplicate search should propagate to the caller."""
        run_id = uuid4()
        ts = datetime.now()
        embedding = [0.1] * 384

        processor.db.session.execute.side_effect = RuntimeError("ScyllaDB timeout")

        with pytest.raises(RuntimeError, match="ScyllaDB timeout"):
            processor._mark_event_is_duplicate(run_id, ts, "ERROR", embedding)


class TestCaching:
    """Tests for the embedding cache used to bridge the vector-search indexing lag."""

    @pytest.fixture
    def processor(self):
        """Create processor with mocked dependencies."""
        with (
            patch("argusAI.event_similarity_processor_v2.ScyllaConnection") as mock_scylla,
            patch("argusAI.event_similarity_processor_v2.BgeSmallEnEmbeddingModel") as mock_embedding,
            patch("argusAI.event_similarity_processor_v2.MessageSanitizer") as mock_sanitizer,
        ):
            mock_db = MagicMock()
            mock_scylla.return_value = mock_db
            mock_embedding.return_value = MagicMock(return_value=[[0.1] * 384])
            mock_sanitizer.return_value = MagicMock(sanitize=MagicMock(return_value="sanitized"))
            processor = EventSimilarityProcessorV2()
            return processor

    def test_processed_event_should_be_added_to_cache(self, processor):
        """A successfully embedded event should appear in similar_event_cache immediately."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        mock_event = Mock(message="Disk full on node")
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        with patch.object(processor, "_mark_event_is_duplicate", return_value=False):
            processor._process_single_event(run_id, severity, ts)

        cache_key = (run_id, severity)
        assert cache_key in processor.similar_event_cache
        cached = processor.similar_event_cache[cache_key]
        assert len(cached) == 1
        assert cached[0].ts == ts

    def test_duplicate_event_should_not_be_added_to_cache(self, processor):
        """An event identified as a duplicate must not be inserted into the cache."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        mock_event = Mock(message="Disk full on node")
        mock_result = Mock()
        mock_result.one.return_value = mock_event
        processor.db.execute.return_value = mock_result

        with patch.object(processor, "_mark_event_is_duplicate", return_value=True):
            processor._process_single_event(run_id, severity, ts)

        assert len(processor.similar_event_cache) == 0

    def test_merge_cache_should_remove_entries_now_visible_in_database(self, processor):
        """Cache entries that the ANN query now returns (indexed) should be evicted from cache."""
        run_id = uuid4()
        severity = "ERROR"
        ts = datetime.now()

        stale = SimilarEvent(run_id=run_id, ts=ts, embedding=[0.1] * 384, added_ts=time.time())
        processor.similar_event_cache[(run_id, severity)] = [stale]

        # Database now returns the same event (vector index has caught up)
        combined_rows, remaining_cache = processor._merge_cache(run_id, severity, [stale])

        # Entry should have been evicted from the in-memory cache
        assert len(remaining_cache) == 0
        assert processor.similar_event_cache[(run_id, severity)] == []
        # But it is still present in the combined view returned to the caller
        assert stale in combined_rows

    def test_merge_cache_should_keep_entries_not_yet_indexed_by_database(self, processor):
        """Cache entries absent from the ANN result set should be preserved and included in combined rows."""
        run_id = uuid4()
        severity = "ERROR"

        unindexed = SimilarEvent(run_id=run_id, ts=datetime.now(), embedding=[0.2] * 384, added_ts=time.time())
        processor.similar_event_cache[(run_id, severity)] = [unindexed]

        # DB returns a different event (unindexed is not visible there yet)
        db_row = SimilarEvent(run_id=run_id, ts=datetime.now(), embedding=[0.5] * 384, added_ts=time.time())

        combined_rows, remaining_cache = processor._merge_cache(run_id, severity, [db_row])

        # Unindexed cache entry should survive in cache and appear in combined rows
        assert unindexed in remaining_cache
        assert unindexed in combined_rows
        assert db_row in combined_rows

    def test_is_stale_should_return_true_when_event_present_in_db_rows(self, processor):
        """is_stale should identify a cached event that is now returned by the database."""
        run_id = uuid4()
        ts = datetime.now()
        cached = SimilarEvent(run_id=run_id, ts=ts, embedding=[0.1] * 384, added_ts=time.time())
        db_row = SimilarEvent(run_id=run_id, ts=ts, embedding=[0.1] * 384, added_ts=time.time())

        assert processor.is_stale(cached, [db_row]) is True

    def test_is_stale_should_return_false_when_event_absent_from_db_rows(self, processor):
        """is_stale should return False when the cached event is not yet returned by the database."""
        run_id = uuid4()
        cached = SimilarEvent(run_id=run_id, ts=datetime.now(), embedding=[0.1] * 384, added_ts=time.time())
        other_row = SimilarEvent(run_id=run_id, ts=datetime.now(), embedding=[0.5] * 384, added_ts=time.time())

        assert processor.is_stale(cached, [other_row]) is False

    def test_clear_stale_cache_should_remove_keys_whose_entries_have_expired_added_ts(self, processor):
        """Cache entries with an old added_ts (not in the future window) should be purged."""
        run_id = uuid4()
        severity = "ERROR"

        # added_ts = 0.0 is far in the past; cmp_ts returns False → full_cache empty → key dropped
        old_entry = SimilarEvent(run_id=run_id, ts=datetime.now(), embedding=[0.1] * 384, added_ts=0.0)
        processor.similar_event_cache[(run_id, severity)] = [old_entry]

        processor._clear_stale_cache()

        assert (run_id, severity) not in processor.similar_event_cache

    def test_clear_stale_cache_should_retain_keys_with_future_added_ts(self, processor):
        """Cache entries whose added_ts lies beyond time.time() + cache_clear_timer should be retained."""
        run_id = uuid4()
        severity = "ERROR"

        # added_ts > time.time() + cache_clear_timer so the entry survives
        future_ts = time.time() + processor.cache_clear_timer + 60
        fresh_entry = SimilarEvent(run_id=run_id, ts=datetime.now(), embedding=[0.1] * 384, added_ts=future_ts)
        processor.similar_event_cache[(run_id, severity)] = [fresh_entry]

        processor._clear_stale_cache()

        assert (run_id, severity) in processor.similar_event_cache
        assert len(processor.similar_event_cache[(run_id, severity)]) == 1

    def test_deduplication_should_use_cache_when_db_has_not_indexed_new_event(self, processor):
        """An event stored in cache (not yet in DB) should still trigger deduplication for a new identical event."""
        run_id = uuid4()
        severity = "ERROR"
        ts_existing = datetime.now()
        existing_event_id = uuid4()
        # Identical unit vectors
        embedding = [1.0] + [0.0] * 383

        # Pre-populate cache; DB ANN query returns nothing (not yet indexed)
        cached_event = SimilarEvent(run_id=run_id, ts=ts_existing, embedding=embedding, added_ts=time.time() + 3600)
        processor.similar_event_cache[(run_id, severity)] = [cached_event]

        mock_dupe = Mock(event_id=existing_event_id)
        ann_call_done = False

        def session_execute_side_effect(*args, **kwargs):
            nonlocal ann_call_done
            if not ann_call_done:
                ann_call_done = True
                return []  # ANN search sees nothing yet
            return Mock(one=Mock(return_value=mock_dupe))

        processor.db.session.execute.side_effect = session_execute_side_effect

        new_ts = datetime.now()
        result = processor._mark_event_is_duplicate(run_id, new_ts, severity, embedding)

        assert result is True
